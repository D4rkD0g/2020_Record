
## 0x01 白话强化学习与Pytorch

微信读书里有，学习强化学习的知识，摘抄了一些知识点

第六章 深度学习  
> 在“传统篇”中，我们讨论的内容主要是：如何靠谱地估算一个状态的价值或者一个状态下一个动作的价值，如何通过贝尔曼方程对奖励值进行有效回溯，以及如何通过统计学的手段（例如，多次测量取平均值）来逐步把这个价值估算准确。可以说，这一系列问题都是在处理低维度的State数据问题，也可以说，只是在理论上讨论这样的问题应该如何解决

> 卷积核的好处显而易见。它可以作为特征提取的工具，相对于全连接网络来说，能够有效降低过拟合的危害、提高泛化性、减少参数的数量。还有池化层（Pooling Layer），这种层可以对输入的图像（或者特征图）进行降采样（Subsampling）处理，也就是缩小它的尺寸，从而在一定程度上继续提高泛化性、减少参数的数量

> 神经元通常由两部分组成，一是“线性模型”部分，二是“激励函数”

Hyper Parameters   
```python
input_size =1
output_size = 1
num_epochs = 600
learning_rate = 0.001
```

使用PyTorch的nn包中的Linear类，并实例化  
```python
class LinearRegression(nn.Module):
    def __init__(self, input_size, output_size):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(input_size, output_size)
    def forward(self, x):
        out = self.linear(x)
        return out

model = LinearRegression(input_size, output_size)
```

nn.MSELoss：均方损失函数，通常是用来解决回归问题。如果模型输出的是连续的实数值或者向量空间值，那么用MSE Loss来描述损失比较合适    
torch.optim.SGD：随机梯度下降（StochasticGradient Descent, SGD）更新待定系数，让它向着减小损失函数的值的方向变化  

激励函数  
引入非线性因素，防止过拟合或者欠拟合   
Sigmoid函数(0, 1)  
Tanh函数：多用于RNN，(-1,1)  
ReLU函数：多用于CNN，max(x, 0)  

一个神经网络，通常会分成输入层(Input Layer)、隐藏层(Hidden Layer)、输出层(Output Layer)  
深度残差网络 ResNet  

明天继续

## 0x02 某办公通信工具  

安装包里为什么还提供了pdb文件。。。  

IDA Pro 7.0动态调试Win32位程序时，出现Oops! internal error 1491 occured。。。  
1. 升级IDA版本至7.0sp1或以上  
2. 或者使用远程调试，win32_remote.exe   

用的第二种方法可行  

本想用Python的ctypes来加载dll库，试一下，结果python是64位的，而目标dll是32位的，不能直接加载   

